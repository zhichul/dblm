{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf10c011-1461-4595-b089-447dfa534140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"HF_HOME\"]=\"/export/a02/huggingface\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047d79f6-dad9-4326-ab83-8b22453b33fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7b98b6-ea2a-4646-8f7a-1383fb488793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize(tokenizer, chunks: List[List[str]], weights: List[Optional[List[float]]], log_marginals_dtype=torch.float):\n",
    "    if tokenizer.add_bos_token == True:\n",
    "        raise ValueError()\n",
    "    tokens = tokenizer(sum(chunks, []), return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    ser_input_ids = tokens[\"input_ids\"]\n",
    "    ser_attention_mask = tokens[\"attention_mask\"]\n",
    "    length = ser_attention_mask.sum()\n",
    "    input_ids = ser_input_ids.new_zeros(length + 1)\n",
    "    min = torch.finfo(log_marginals_dtype).min\n",
    "    log_marginals = ser_input_ids.new_zeros((length + 1, length + 1), dtype=log_marginals_dtype).fill_(min).triu(diagonal=1)\n",
    "    attention_mask = torch.tril(ser_input_ids.new_ones((length + 1, length + 1)))\n",
    "    serial_index = 0\n",
    "    chunk_offset = 1\n",
    "    alt_offset = 1\n",
    "    input_ids[0] = tokenizer.bos_token_id\n",
    "    for alts, ws in zip(chunks, weights):\n",
    "        ws = ws if ws is not None else [0.0]\n",
    "        if (len(alts) != len(ws)):\n",
    "            raise ValueError(f\"chunks length and weight length must match. chunk\"\n",
    "                            f\" {serial_index} erred.\\n{chunks}\\n{weights}\")\n",
    "        for alt, w in zip(alts, log_marginals.new_tensor(ws).log_softmax(-1)):\n",
    "            l = ser_attention_mask[serial_index].sum()\n",
    "            input_ids[alt_offset:alt_offset+l] = ser_input_ids[serial_index][:l]\n",
    "            attention_mask[alt_offset:alt_offset+l, chunk_offset:alt_offset] = 0\n",
    "            log_marginals[alt_offset:alt_offset+l, chunk_offset:alt_offset] = min\n",
    "            log_marginals[alt_offset+l:, alt_offset:alt_offset+l] = w\n",
    "            alt_offset += l\n",
    "            serial_index += 1\n",
    "        chunk_offset = alt_offset\n",
    "    return input_ids, attention_mask, log_marginals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9049349-3dc4-4620-8505-b8db9916cec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf289cf22fc14aa1974e3efb5943ec62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", add_bos_token=False, pad_token=\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52812868-1ece-490e-8e3c-bd78a4fc4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that adheres to the format required by the user.\"\n",
    "user_prompt = \"Please answer the following question about an imagined uncertain situation:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38580d6f-962e-45fa-8c9f-5386600d1cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768bb716f3b24d9c8d03a4b397097398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Run 0 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is 15, while another possibility is 75. As I cannot determine the correct age with absolute certainty, I will provide a range of possible ages for Alice:\n",
      "\n",
      "Age of Alice: 15-75 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 1 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is 75 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 2 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is:\n",
      "\n",
      "42\n",
      "****** ******\n",
      "\n",
      "****** Run 3 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is:\n",
      "\n",
      "Alice is 50 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 4 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is: 32 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 5 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is 15 years old, and another possibility is 75 years old. Therefore, Alice's age is uncertain and can be either 15 or 75.\n",
      "****** ******\n",
      "\n",
      "****** Run 6 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is:\n",
      "\n",
      "Alice is 15 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 7 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is 15 years old. Therefore, I would answer \"15\" as the age of Alice.\n",
      "****** ******\n",
      "\n",
      "****** Run 8 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is 15 years old.\n",
      "****** ******\n",
      "\n",
      "****** Run 9 ******\n",
      "[INST] <<SYS>>\n",
      "You are a helpful assistant that adheres to the format required by the user.\n",
      "<</SYS>>\n",
      "Please answer the following question about an imagined uncertain situation:\n",
      "\n",
      " Alice is 15 years old or Alice is 75 years old. \n",
      "\n",
      "How old is Alice? Please answer in a single number.[/INST] Given the provided information, one possibility of Alice's age is: 32 years old (15 + 75).\n",
      "****** ******\n",
      "\n",
      "[15, 15, 15, 15, 15, 32, 32, 42, 50, 75]\n",
      "30.6\n",
      "23.5\n",
      "19.23122460999299\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "\n",
    "options = [\"Alice is 15 years old or\", \"Alice is 75 years old.\", ]\n",
    "weights = [0.0, 0.0]\n",
    "question = \"How old is Alice? Please answer in a single number.\"\n",
    "answer_prefix = \"Given the provided information, one possibility of Alice's age is\"\n",
    "input_ids, attention_mask, log_marginals = serialize(tokenizer, [[f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n{user_prompt}\\n\\n\"], options,[f\"\\n\\n{question}[/INST] {answer_prefix}\"]], [None, weights, None], log_marginals_dtype=torch.float16)\n",
    "print(input_ids.size())\n",
    "results = []\n",
    "for i in tqdm(range(N)):\n",
    "    print(f\"****** Run {i} ******\")\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=128,\n",
    "        max_new_tokens=128,\n",
    "        temperature=1.0,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        use_cache=True\n",
    "    )\n",
    "    input_ids.size(-1)\n",
    "    outputs = model.generate(input_ids=input_ids[None, ...], generation_config=generation_config, attention_mode=\"mult\", log_marginals=log_marginals[None, ...])\n",
    "    generated = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    print(generated)\n",
    "    new = tokenizer.batch_decode(outputs[:, input_ids.size(-1):], skip_special_tokens=True)[0]\n",
    "    try:\n",
    "        results.append(int(re.search(r\"[^\\d]*(\\d+)[^\\d]*\", new).group(1)))\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"****** ******\")\n",
    "    print()\n",
    "print(sorted(results))\n",
    "print(np.mean(results))\n",
    "print(np.median(results))\n",
    "print(np.std(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670c68d-4b9a-4997-9e97-cc43d128f8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft24",
   "language": "python",
   "name": "peft24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
